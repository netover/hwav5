"""
Test script for NVIDIA LLM API using LiteLLM
"""

import asyncio
import os

from dotenv import load_dotenv

# Load environment variables
load_dotenv()


async def test_litellm_nvidia():
    """Test NVIDIA LLM API using LiteLLM"""
    try:
        # Import litellm
        from litellm import acompletion

        # Set NVIDIA API key
        os.environ["NVIDIA_API_KEY"] = (
            "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"
        )

        print("üöÄ Testing NVIDIA LLM API with LiteLLM...")
        print("üì° Using model: nvidia/llama-3.3-nemotron-super-49b-v1.5")
        print("=" * 50)

        # Create completion request
        response = await acompletion(
            model="nvidia/nemotron-super-49b-v1",  # Using the model prefix for NVIDIA provider
            messages=[
                {
                    "role": "system",
                    "content": "Voc√™ √© um assistente √∫til que responde em portugu√™s de forma clara e concisa.",
                },
                {
                    "role": "user",
                    "content": "Ol√°! Por favor, me diga como est√° o sistema Resync TWS Integration.",
                },
            ],
            temperature=0.6,
            top_p=0.95,
            max_tokens=500,
            frequency_penalty=0,
            presence_penalty=0,
        )

        # Print response
        print("‚úÖ API Response received:")
        print(response.choices[0].message.content)
        print("=" * 50)
        print("üéâ NVIDIA LLM API with LiteLLM is working correctly!")
        return True

    except Exception as e:
        print(f"‚ùå Error testing NVIDIA LLM API with LiteLLM: {e}")
        return False


async def test_litellm_streaming():
    """Test streaming response using LiteLLM"""
    try:
        # Import litellm streaming
        from litellm import acompletion

        # Set NVIDIA API key
        os.environ["NVIDIA_API_KEY"] = (
            "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"
        )

        print("\nüåä Testing streaming response with LiteLLM...")
        print("=" * 50)

        # Create streaming completion request
        response = await acompletion(
            model="nvidia/nemotron-super-49b-v1",
            messages=[
                {
                    "role": "system",
                    "content": "Voc√™ √© um assistente √∫til que responde em portugu√™s.",
                },
                {"role": "user", "content": "Liste 3 benef√≠cios principais do sistema Resync."},
            ],
            temperature=0.6,
            top_p=0.95,
            max_tokens=300,
            frequency_penalty=0,
            presence_penalty=0,
            stream=True,
        )

        # Print streaming response
        print("‚úÖ Streaming response:")
        full_response = ""
        async for chunk in response:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                print(content, end="")
                full_response += content

        print("\n" + "=" * 50)
        print("üéâ Streaming response with LiteLLM completed successfully!")
        return True

    except Exception as e:
        print(f"‚ùå Error testing streaming with LiteLLM: {e}")
        return False


async def main():
    """Main test function"""
    print("üß™ NVIDIA LLM API Test Suite (LiteLLM)")
    print("=" * 50)

    # Test basic completion
    basic_test = await test_litellm_nvidia()

    # Test streaming
    streaming_test = await test_litellm_streaming()

    # Summary
    print("\nüìä Test Summary:")
    print(f"Basic completion: {'‚úÖ PASS' if basic_test else '‚ùå FAIL'}")
    print(f"Streaming: {'‚úÖ PASS' if streaming_test else '‚ùå FAIL'}")

    if basic_test and streaming_test:
        print("\nüéØ All tests passed! NVIDIA LLM with LiteLLM is ready for integration.")
    else:
        print("\n‚ö†Ô∏è  Some tests failed. Please check the configuration.")


if __name__ == "__main__":
    asyncio.run(main())
