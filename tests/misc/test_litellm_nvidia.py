"""
Test script for NVIDIA LLM API using LiteLLM
"""
import asyncio
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

async def test_litellm_nvidia():
    """Test NVIDIA LLM API using LiteLLM"""
    try:
        # Import litellm
        from litellm import acompletion
        
        # Set NVIDIA API key
        os.environ["NVIDIA_API_KEY"] = "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"
        
        print("ğŸš€ Testing NVIDIA LLM API with LiteLLM...")
        print(f"ğŸ“¡ Using model: nvidia/llama-3.3-nemotron-super-49b-v1.5")
        print("=" * 50)

        # Create completion request
        response = await acompletion(
            model="nvidia/nemotron-super-49b-v1",  # Using the model prefix for NVIDIA provider
            messages=[
                {"role": "system", "content": "VocÃª Ã© um assistente Ãºtil que responde em portuguÃªs de forma clara e concisa."},
                {"role": "user", "content": "OlÃ¡! Por favor, me diga como estÃ¡ o sistema Resync TWS Integration."}
            ],
            temperature=0.6,
            top_p=0.95,
            max_tokens=500,
            frequency_penalty=0,
            presence_penalty=0
        )

        # Print response
        print("âœ… API Response received:")
        print(response.choices[0].message.content)
        print("=" * 50)
        print("ğŸ‰ NVIDIA LLM API with LiteLLM is working correctly!")
        return True

    except Exception as e:
        print(f"âŒ Error testing NVIDIA LLM API with LiteLLM: {e}")
        return False

async def test_litellm_streaming():
    """Test streaming response using LiteLLM"""
    try:
        # Import litellm streaming
        from litellm import acompletion
        
        # Set NVIDIA API key
        os.environ["NVIDIA_API_KEY"] = "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"
        
        print("\nğŸŒŠ Testing streaming response with LiteLLM...")
        print("=" * 50)

        # Create streaming completion request
        response = await acompletion(
            model="nvidia/nemotron-super-49b-v1",
            messages=[
                {"role": "system", "content": "VocÃª Ã© um assistente Ãºtil que responde em portuguÃªs."},
                {"role": "user", "content": "Liste 3 benefÃ­cios principais do sistema Resync."}
            ],
            temperature=0.6,
            top_p=0.95,
            max_tokens=300,
            frequency_penalty=0,
            presence_penalty=0,
            stream=True
        )

        # Print streaming response
        print("âœ… Streaming response:")
        full_response = ""
        async for chunk in response:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                print(content, end="")
                full_response += content
        
        print("\n" + "=" * 50)
        print("ğŸ‰ Streaming response with LiteLLM completed successfully!")
        return True

    except Exception as e:
        print(f"âŒ Error testing streaming with LiteLLM: {e}")
        return False

async def main():
    """Main test function"""
    print("ğŸ§ª NVIDIA LLM API Test Suite (LiteLLM)")
    print("=" * 50)
    
    # Test basic completion
    basic_test = await test_litellm_nvidia()
    
    # Test streaming
    streaming_test = await test_litellm_streaming()
    
    # Summary
    print("\nğŸ“Š Test Summary:")
    print(f"Basic completion: {'âœ… PASS' if basic_test else 'âŒ FAIL'}")
    print(f"Streaming: {'âœ… PASS' if streaming_test else 'âŒ FAIL'}")
    
    if basic_test and streaming_test:
        print("\nğŸ¯ All tests passed! NVIDIA LLM with LiteLLM is ready for integration.")
    else:
        print("\nâš ï¸  Some tests failed. Please check the configuration.")

if __name__ == "__main__":
    asyncio.run(main())
