"""
pgvector-based vector store with Binary+Halfvec optimization.

Implements the gold standard for high-performance vector search:
- Binary HNSW for ultra-fast initial search (~5ms)
- Halfvec cosine for precise rescoring (~10ms)
- Auto-quantize trigger keeps Python code simple

Storage: ~75% reduction vs float32
Speed: ~70% faster search
Quality: ~99% with halfvec rescoring

Author: Resync Team
Version: 5.9.0
"""

import logging
from typing import Any

from resync.knowledge.config import CFG
from resync.knowledge.interfaces import VectorStore

logger = logging.getLogger(__name__)

# Optional imports
try:
    import asyncpg
    ASYNCPG_AVAILABLE = True
except ImportError:
    asyncpg = None
    ASYNCPG_AVAILABLE = False


class PgVectorStore(VectorStore):
    """
    PostgreSQL vector store with Binary+Halfvec optimization.

    Uses two-phase search for optimal speed and accuracy:
    1. Binary HNSW (Hamming distance) for fast candidates
    2. Halfvec cosine similarity for precise rescoring

    The trigger 'trg_auto_quantize_embedding' automatically populates
    embedding_half when embedding is inserted, so Python code stays simple.
    """

    def __init__(
        self,
        database_url: str | None = None,
        collection: str | None = None,
        dim: int = CFG.embed_dim,
    ):
        if not ASYNCPG_AVAILABLE:
            raise RuntimeError("asyncpg is required. pip install asyncpg")

        import os
        self._database_url = database_url or os.getenv("DATABASE_URL") or CFG.database_url

        # Clean URL for asyncpg
        if self._database_url.startswith("postgresql+asyncpg://"):
            self._database_url = self._database_url.replace(
                "postgresql+asyncpg://", "postgresql://"
            )

        self._collection_default = collection or CFG.collection_write
        self._dim = dim
        self._pool: asyncpg.Pool | None = None
        self._initialized = False

    async def _get_pool(self) -> asyncpg.Pool:
        """Get or create connection pool."""
        if self._pool is None:
            self._pool = await asyncpg.create_pool(
                self._database_url,
                min_size=2,
                max_size=10,
                command_timeout=60.0,
            )
        return self._pool

    async def close(self) -> None:
        """Close connection pool."""
        if self._pool:
            await self._pool.close()
            self._pool = None

    # =========================================================================
    # UPSERT OPERATIONS
    # =========================================================================

    async def upsert_batch(
        self,
        ids: list[str],
        vectors: list[list[float]],
        payloads: list[dict[str, Any]],
        collection: str | None = None,
    ) -> None:
        """
        Batch upsert documents with embeddings.

        Note: The trigger 'trg_auto_quantize_embedding' automatically
        populates embedding_half from embedding.
        """
        if not ids:
            return

        col = collection or self._collection_default
        pool = await self._get_pool()

        records = []
        for doc_id, vector, payload in zip(ids, vectors, payloads, strict=False):
            content = payload.get("text", payload.get("content", ""))
            sha256 = payload.get("sha256", "")
            chunk_id = payload.get("chunk_id", 0)

            embedding_str = f"[{','.join(str(x) for x in vector)}]"

            metadata = {
                k: v for k, v in payload.items()
                if k not in ("text", "content", "sha256", "chunk_id")
            }

            records.append((
                col, doc_id, chunk_id, content, embedding_str,
                metadata, sha256
            ))

        async with pool.acquire() as conn:
            # Use executemany for efficiency
            await conn.executemany("""
                INSERT INTO document_embeddings
                (collection_name, document_id, chunk_id, content, embedding, metadata, sha256)
                VALUES ($1, $2, $3, $4, $5::vector, $6::jsonb, $7)
                ON CONFLICT (collection_name, document_id, chunk_id)
                DO UPDATE SET
                    content = EXCLUDED.content,
                    embedding = EXCLUDED.embedding,
                    metadata = EXCLUDED.metadata,
                    sha256 = EXCLUDED.sha256,
                    updated_at = CURRENT_TIMESTAMP
            """, records)

        logger.debug("batch_upserted", collection=col, count=len(ids))

    # =========================================================================
    # QUERY OPERATIONS - OPTIMIZED TWO-PHASE SEARCH
    # =========================================================================

    async def query(
        self,
        vector: list[float],
        top_k: int,
        collection: str | None = None,
        filters: dict[str, Any] | None = None,
        with_vectors: bool = False,
    ) -> list[dict[str, Any]]:
        """
        Query using optimized two-phase Binary+Halfvec search.

        Phase 1: Binary HNSW (Hamming) for fast candidates (~5ms)
        Phase 2: Halfvec cosine for precise rescoring (~10ms)
        """
        col = collection or CFG.collection_read
        pool = await self._get_pool()

        # Convert vector to formats needed
        embedding_str = f"[{','.join(str(x) for x in vector)}]"
        binary_str = "".join("1" if v > 0 else "0" for v in vector)

        # Build filter clause
        filter_clause = ""
        filter_params = []
        param_idx = 4  # $1=embedding, $2=binary, $3=collection

        if filters:
            for key, value in filters.items():
                if value is None:
                    continue
                if key == "sha256":
                    filter_clause += f" AND sha256 = ${param_idx}"
                else:
                    filter_clause += f" AND metadata->>'{key}' = ${param_idx}"
                filter_params.append(str(value))
                param_idx += 1

        # Calculate candidates (10x for good recall)
        candidates = max(top_k * 10, 50)

        # Two-phase query
        query = f"""
            WITH binary_candidates AS (
                -- Phase 1: Fast binary search
                SELECT id, document_id, chunk_id, content, metadata, sha256, embedding_half
                FROM document_embeddings
                WHERE collection_name = $3
                {filter_clause}
                ORDER BY binary_quantize(embedding_half)::bit({self._dim}) <~> $2::bit({self._dim})
                LIMIT {candidates}
            )
            -- Phase 2: Precise halfvec rescoring
            SELECT
                document_id, chunk_id, content, metadata, sha256,
                1 - (embedding_half <=> $1::halfvec) AS similarity
            FROM binary_candidates
            ORDER BY embedding_half <=> $1::halfvec
            LIMIT ${param_idx}
        """

        params = [embedding_str, binary_str, col, *filter_params, top_k]

        async with pool.acquire() as conn:
            rows = await conn.fetch(query, *params)

        results = []
        for row in rows:
            payload = dict(row["metadata"]) if row["metadata"] else {}
            payload["text"] = row["content"]
            payload["sha256"] = row["sha256"]
            payload["chunk_id"] = row["chunk_id"]

            results.append({
                "id": row["document_id"],
                "score": float(row["similarity"]),
                "payload": payload,
            })

        logger.debug(
            "query_completed",
            collection=col,
            results=len(results),
            candidates=candidates,
            mode="binary_halfvec"
        )

        return results

    async def query_simple(
        self,
        vector: list[float],
        top_k: int,
        collection: str | None = None,
    ) -> list[dict[str, Any]]:
        """
        Simple halfvec-only search (faster, slightly less accurate).

        Use this when speed is critical and you don't need maximum precision.
        """
        col = collection or CFG.collection_read
        pool = await self._get_pool()

        embedding_str = f"[{','.join(str(x) for x in vector)}]"

        query = """
            SELECT
                document_id, chunk_id, content, metadata, sha256,
                1 - (embedding_half <=> $1::halfvec) AS similarity
            FROM document_embeddings
            WHERE collection_name = $2
            ORDER BY embedding_half <=> $1::halfvec
            LIMIT $3
        """

        async with pool.acquire() as conn:
            rows = await conn.fetch(query, embedding_str, col, top_k)

        results = []
        for row in rows:
            payload = dict(row["metadata"]) if row["metadata"] else {}
            payload["text"] = row["content"]
            payload["sha256"] = row["sha256"]
            payload["chunk_id"] = row["chunk_id"]

            results.append({
                "id": row["document_id"],
                "score": float(row["similarity"]),
                "payload": payload,
            })

        return results

    # =========================================================================
    # UTILITY OPERATIONS
    # =========================================================================

    async def count(self, collection: str | None = None) -> int:
        """Count documents in collection."""
        col = collection or self._collection_default
        pool = await self._get_pool()

        async with pool.acquire() as conn:
            count = await conn.fetchval(
                "SELECT COUNT(*) FROM document_embeddings WHERE collection_name = $1",
                col
            )
        return count or 0

    async def exists(self, document_id: str, collection: str | None = None) -> bool:
        """Check if document exists."""
        col = collection or self._collection_default
        pool = await self._get_pool()

        async with pool.acquire() as conn:
            exists = await conn.fetchval(
                """
                SELECT 1 FROM document_embeddings
                WHERE collection_name = $1 AND document_id = $2
                LIMIT 1
                """,
                col, document_id
            )
        return exists is not None

    async def delete(
        self,
        document_id: str,
        collection: str | None = None,
    ) -> bool:
        """Delete document by ID."""
        col = collection or self._collection_default
        pool = await self._get_pool()

        async with pool.acquire() as conn:
            result = await conn.execute(
                """
                DELETE FROM document_embeddings
                WHERE collection_name = $1 AND document_id = $2
                """,
                col, document_id
            )

        deleted = result.split()[-1]
        return int(deleted) > 0

    async def delete_collection(self, collection: str) -> int:
        """Delete entire collection."""
        pool = await self._get_pool()

        async with pool.acquire() as conn:
            result = await conn.execute(
                "DELETE FROM document_embeddings WHERE collection_name = $1",
                collection
            )

        deleted = result.split()[-1]
        return int(deleted)

    async def get_stats(self, collection: str | None = None) -> dict[str, Any]:
        """Get collection statistics."""
        col = collection or self._collection_default
        pool = await self._get_pool()

        async with pool.acquire() as conn:
            stats = await conn.fetchrow(
                """
                SELECT
                    COUNT(*) as document_count,
                    COUNT(DISTINCT document_id) as unique_documents,
                    MIN(created_at) as oldest_doc,
                    MAX(updated_at) as newest_doc,
                    pg_size_pretty(pg_total_relation_size('document_embeddings')) as table_size
                FROM document_embeddings
                WHERE collection_name = $1
                """,
                col
            )

        return {
            "collection": col,
            "document_count": stats["document_count"],
            "unique_documents": stats["unique_documents"],
            "oldest_doc": stats["oldest_doc"].isoformat() if stats["oldest_doc"] else None,
            "newest_doc": stats["newest_doc"].isoformat() if stats["newest_doc"] else None,
            "table_size": stats["table_size"],
            "search_mode": "binary_halfvec",
        }

    async def get_all_documents(
        self,
        collection: str | None = None,
        limit: int = 10000,
    ) -> list[dict[str, Any]]:
        """
        Retrieve all documents from the vector store.
        
        Required by VectorStore interface for BM25 index building.
        
        Args:
            collection: Collection name (optional, uses default)
            limit: Maximum number of documents to retrieve
            
        Returns:
            List of documents with content and metadata
        """
        col = collection or self._collection_default
        pool = await self._get_pool()

        async with pool.acquire() as conn:
            rows = await conn.fetch(
                """
                SELECT 
                    document_id,
                    chunk_index,
                    content,
                    metadata,
                    created_at,
                    updated_at
                FROM document_embeddings
                WHERE collection_name = $1
                ORDER BY document_id, chunk_index
                LIMIT $2
                """,
                col,
                limit
            )

        documents = []
        for row in rows:
            doc = {
                "id": f"{row['document_id']}_{row['chunk_index']}",
                "document_id": row["document_id"],
                "chunk_index": row["chunk_index"],
                "content": row["content"],
                "metadata": row["metadata"] or {},
                "created_at": row["created_at"].isoformat() if row["created_at"] else None,
                "updated_at": row["updated_at"].isoformat() if row["updated_at"] else None,
            }
            documents.append(doc)

        logger.info(f"Retrieved {len(documents)} documents from collection '{col}'")
        return documents


# =============================================================================
# Singleton accessor
# =============================================================================

_store_instance: PgVectorStore | None = None


async def get_vector_store() -> PgVectorStore:
    """Get singleton vector store instance."""
    global _store_instance

    if _store_instance is None:
        _store_instance = PgVectorStore()

    return _store_instance
