"""
LLM Service using OpenAI SDK for OpenAI-Compatible APIs.

This service uses the OpenAI Python SDK to connect to any OpenAI-compatible API:
- NVIDIA NIM (tested and confirmed)
- Azure OpenAI
- Local models (vLLM, ollama with OpenAI mode)
- OpenAI directly

NOTE: This does NOT use LiteLLM directly. If multi-provider support with automatic
      fallback is needed, consider migrating to `from litellm import acompletion`.

Now integrated with LangFuse for:
- Prompt management (externalized prompts)
- Observability and tracing
- Cost tracking

Usage:
    from resync.services.llm_service import get_llm_service

    llm = get_llm_service()
    response = await llm.generate_agent_response(
        agent_id="tws-agent",
        user_message="Quais jobs estão em ABEND?",
    )

Configuration (settings):
    - llm_model: Model name (e.g., "meta/llama-3.1-70b-instruct")
    - llm_endpoint: Base URL (e.g., "https://integrate.api.nvidia.com/v1")
    - llm_api_key: API key (SecretStr supported)
"""

from __future__ import annotations

import logging
from collections.abc import AsyncGenerator
from functools import lru_cache
from typing import Any

from resync.core.exceptions import IntegrationError
from resync.core.utils.prompt_formatter import OpinionBasedPromptFormatter
from resync.settings import settings

# LangFuse integration (optional but recommended)
try:
    from resync.core.langfuse import (
        PromptType,
        get_prompt_manager,
        get_tracer,
    )

    LANGFUSE_INTEGRATION = True
except ImportError:
    LANGFUSE_INTEGRATION = False

try:
    # Import specific exceptions from OpenAI v1.x
    from openai import (
        APIConnectionError,
        APIError,
        APIStatusError,
        APITimeoutError,
        AsyncOpenAI,
        AuthenticationError,
        BadRequestError,
        RateLimitError,
    )

    OPENAI_AVAILABLE = True
except ImportError:  # pragma: no cover
    OPENAI_AVAILABLE = False

logger = logging.getLogger(__name__)


def _coerce_secret(value: Any) -> str | None:
    """Accept str or pydantic SecretStr; return plain str or None."""
    if value is None:
        return None
    if isinstance(value, str):
        return value
    # pydantic SecretStr compatibility
    get_secret = getattr(value, "get_secret_value", None)
    return get_secret() if callable(get_secret) else str(value)


class LLMService:
    """Service for interacting with LLM APIs through OpenAI-compatible endpoints."""

    def __init__(self) -> None:
        """Initialize LLM service with NVIDIA API configuration"""
        if not OPENAI_AVAILABLE:
            raise IntegrationError(
                message="openai package is required but not installed",
                details={"install_command": "pip install openai"},
            )

        # --- Model resolution with fallbacks ---
        model = getattr(settings, "llm_model", None)
        if model is None:
            model = getattr(settings, "agent_model_name", None)
        if not model:
            raise IntegrationError(
                message="No LLM model configured",
                details={"hint": "Define settings.llm_model or settings.agent_model_name"},
            )
        self.model: str = str(model)

        # Defaults
        self.default_temperature = 0.6
        self.default_top_p = 0.95
        self.default_max_tokens = 1000
        self.default_frequency_penalty = 0.0
        self.default_presence_penalty = 0.0

        # --- API key / endpoint (NVIDIA OpenAI-compatible) ---
        api_key = _coerce_secret(getattr(settings, "llm_api_key", None))
        base_url = getattr(settings, "llm_endpoint", None)
        if not base_url:
            raise IntegrationError(
                message="Missing LLM base_url",
                details={"hint": "Configure settings.llm_endpoint (NVIDIA OpenAI-compatible)"},
            )

        if api_key:
            masked = (api_key[:4] + "...") if len(api_key) > 4 else "***"
            logger.info("Using LLM API key: %s", masked)
        else:
            logger.info("No LLM API key configured")
        logger.info("LLM base URL: %s", base_url)

        try:
            self.client = AsyncOpenAI(
                api_key=api_key,
                base_url=base_url,
                timeout=20.0,  # Default do SDK é 10 min; produção costuma usar valores menores
                max_retries=2,  # README mostra como ajustar (global/per-request)
            )
            logger.info("LLM service initialized with model: %s", self.model)
            
            # Opinion-Based Prompting for +30-50% context adherence improvement
            self._prompt_formatter = OpinionBasedPromptFormatter()
            logger.debug("OpinionBasedPromptFormatter initialized for enhanced RAG accuracy")
        except (
            AuthenticationError,
            RateLimitError,
            APIConnectionError,
            BadRequestError,
            APIError,
            APITimeoutError,
            APIStatusError,
        ) as exc:
            logger.error("Failed to initialize LLM service (OpenAI error): %s", exc, exc_info=True)
            raise IntegrationError(
                message="Failed to initialize LLM service",
                details={
                    "error": str(exc),
                    "request_id": getattr(exc, "request_id", None),
                },
            ) from exc
        except Exception as exc:  # pylint: disable=broad-exception-caught
            logger.error("Failed to initialize LLM service: %s", exc, exc_info=True)
            raise IntegrationError(
                message="Failed to initialize LLM service",
                details={"error": str(exc)},
            ) from exc

    async def generate_response_with_tools(
        self,
        messages: list[dict[str, str]],
        user_role: str = "operator",
        user_id: str | None = None,
        session_id: str | None = None,
        max_tool_iterations: int = 5,
        temperature: float | None = None,
        max_tokens: int | None = None,
    ) -> str:
        """
        Gera resposta com suporte a tools (function calling).

        Args:
            messages: Mensagens da conversa
            user_role: Role do usuário (para permissões)
            user_id: ID do usuário (para audit)
            session_id: ID da sessão
            max_tool_iterations: Máximo de iterações tool → LLM
            temperature: Temperatura (padrão: 0.3 para tool calls)
            max_tokens: Tokens máximos

        Returns:
            Resposta final do LLM
        """
        try:
            from resync.tools.llm_tools import execute_tool_call, get_llm_tools
            from resync.tools.registry import UserRole
            import json

            # Mapear string role para UserRole enum
            role_map = {
                "viewer": UserRole.VIEWER,
                "operator": UserRole.OPERATOR,
                "admin": UserRole.ADMIN,
                "system": UserRole.SYSTEM,
            }
            user_role_enum = role_map.get(user_role.lower(), UserRole.OPERATOR)

            # Obter tools disponíveis para este user role
            tools = get_llm_tools(user_role=user_role_enum)

            logger.info(
                f"Generating response with {len(tools)} tools available for role {user_role}"
            )

            current_messages = messages.copy()
            temp = temperature if temperature is not None else 0.3
            max_tok = max_tokens if max_tokens is not None else self.default_max_tokens

            for iteration in range(max_tool_iterations):
                # Chamada ao LLM com tools
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=current_messages,
                    tools=tools if tools else None,
                    temperature=temp,
                    max_tokens=max_tok,
                )

                message = response.choices[0].message

                # Se não tem tool calls, retornar resposta
                if not message.tool_calls:
                    return message.content or ""

                # Processar tool calls
                logger.debug(
                    f"LLM requested {len(message.tool_calls)} tool calls at iteration {iteration}"
                )

                # Adicionar mensagem do assistant aos messages
                current_messages.append(
                    {
                        "role": "assistant",
                        "content": message.content,
                        "tool_calls": [
                            {
                                "id": tc.id,
                                "type": "function",
                                "function": {"name": tc.function.name, "arguments": tc.function.arguments},
                            }
                            for tc in message.tool_calls
                        ],
                    }
                )

                # Executar cada tool call
                for tool_call in message.tool_calls:
                    result = await execute_tool_call(
                        tool_call,
                        user_id=user_id,
                        user_role=user_role_enum,
                        session_id=session_id,
                    )

                    # Adicionar resultado como tool message
                    current_messages.append(
                        {
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "name": tool_call.function.name,
                            "content": json.dumps(result, ensure_ascii=False),
                        }
                    )

            # Se chegou aqui, atingiu max_tool_iterations
            logger.warning(f"Max tool iterations ({max_tool_iterations}) reached")

            return "Desculpe, atingi o limite de iterações. Por favor, reformule sua pergunta de forma mais específica."
        except Exception as e:
            logger.error(f"Error in generate_response_with_tools: {e}", exc_info=True)
            # Fallback para resposta sem tools
            return await self.generate_response(messages, temperature=temperature, max_tokens=max_tokens)

    async def generate_response(
        self,
        messages: list[dict[str, str]],
        temperature: float | None = None,
        top_p: float | None = None,
        max_tokens: int | None = None,
        frequency_penalty: float | None = None,
        presence_penalty: float | None = None,
        stream: bool = False,
    ) -> str:
        """
        Generate a response from LLM

        Args:
            messages: List of message dictionaries with 'role' and 'content'
            temperature: Sampling temperature (0.0 to 2.0)
            top_p: Nucleus sampling parameter (0.0 to 1.0)
            max_tokens: Maximum number of tokens to generate
            frequency_penalty: Frequency penalty (-2.0 to 2.0)
            presence_penalty: Presence penalty (-2.0 to 2.0)
            stream: Whether to stream response

        Returns:
            Generated text response
        """
        # pylint: disable=too-many-arguments,too-many-positional-arguments
        # Required for OpenAI API compatibility - all parameters are needed
        temperature = self.default_temperature if temperature is None else temperature
        top_p = self.default_top_p if top_p is None else top_p
        max_tokens = self.default_max_tokens if max_tokens is None else max_tokens
        frequency_penalty = (
            self.default_frequency_penalty if frequency_penalty is None else frequency_penalty
        )
        presence_penalty = (
            self.default_presence_penalty if presence_penalty is None else presence_penalty
        )

        logger.info("Generating LLM response with model: %s", self.model)

        try:
            if stream:
                # Aggregate all chunks and return complete response
                chunks: list[str] = []
                async for piece in self._generate_response_streaming(
                    messages,
                    temperature,
                    top_p,
                    max_tokens,
                    frequency_penalty,
                    presence_penalty,
                ):
                    chunks.append(piece)
                content = "".join(chunks)
            else:
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=temperature,
                    top_p=top_p,
                    max_tokens=max_tokens,  # NVIDIA: keep explicit (known bug)
                    frequency_penalty=frequency_penalty,
                    presence_penalty=presence_penalty,
                    stream=False,
                )
                content = response.choices[0].message.content or ""

            logger.info("Generated LLM response (%d characters)", len(content))
            return content

        except (
            AuthenticationError,
            RateLimitError,
            APIConnectionError,
            BadRequestError,
            APIError,
            APITimeoutError,
            APIStatusError,
        ) as exc:
            logger.error("Error generating LLM response: %s", exc, exc_info=True)
            raise IntegrationError(
                message="Failed to generate LLM response",
                details={
                    "error": str(exc),
                    "request_id": getattr(exc, "request_id", None),
                    "model": self.model,
                },
            ) from exc
        except Exception as exc:  # pylint: disable=broad-exception-caught
            logger.error("Unexpected error generating LLM response: %s", exc, exc_info=True)
            raise IntegrationError(
                message="Failed to generate LLM response",
                details={"error": str(exc), "model": self.model},
            ) from exc

    async def _generate_response_streaming(
        self,
        messages: list[dict[str, str]],
        temperature: float,
        top_p: float,
        max_tokens: int,
        frequency_penalty: float,
        presence_penalty: float,
    ) -> AsyncGenerator[str, None]:
        """
        Generate a streaming response from LLM

        Args:
            messages: List of message dictionaries
            temperature: Sampling temperature
            top_p: Nucleus sampling parameter
            max_tokens: Maximum number of tokens to generate
            frequency_penalty: Frequency penalty
            presence_penalty: Presence penalty

        Yields:
            Chunks of generated response
        """
        # pylint: disable=too-many-arguments,too-many-positional-arguments
        # Required for OpenAI API compatibility - all parameters are needed
        logger.info("Generating streaming LLM response with model: %s", self.model)

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,  # NVIDIA: keep explicit
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
                stream=True,
            )
            async for chunk in response:
                delta = chunk.choices[0].delta
                if getattr(delta, "content", None):
                    yield delta.content  # type: ignore[no-any-return]
        except (
            AuthenticationError,
            RateLimitError,
            APIConnectionError,
            BadRequestError,
            APIError,
            APITimeoutError,
            APIStatusError,
        ) as exc:
            logger.error("Error generating streaming LLM response: %s", exc, exc_info=True)
            raise IntegrationError(
                message="Failed to generate streaming LLM response",
                details={
                    "error": str(exc),
                    "request_id": getattr(exc, "request_id", None),
                    "model": self.model,
                },
            ) from exc
        except Exception as exc:  # pylint: disable=broad-exception-caught
            logger.error(
                "Unexpected error generating streaming LLM response: %s",
                exc,
                exc_info=True,
            )
            raise IntegrationError(
                message="Failed to generate streaming LLM response",
                details={"error": str(exc), "model": self.model},
            ) from exc

    async def generate_agent_response(
        self,
        agent_id: str,
        user_message: str,
        conversation_history: list[dict[str, str]] | None = None,
        agent_config: dict[str, Any] | None = None,
    ) -> str:
        """
        Generate a response from an AI agent.

        Now uses LangFuse prompt management for externalized prompts.
        Falls back to hardcoded prompts if LangFuse is unavailable.

        Args:
            agent_id: ID of agent
            user_message: User's message
            conversation_history: Previous conversation history
            agent_config: Agent configuration

        Returns:
            Generated agent response
        """
        agent_type = (agent_config or {}).get("type", "general")
        agent_name = (agent_config or {}).get("name", f"Agente {agent_id}")
        agent_description = (agent_config or {}).get("description", f"Assistente {agent_type}")

        # Try to get prompt from LangFuse/PromptManager
        system_message = None

        if LANGFUSE_INTEGRATION:
            try:
                prompt_manager = get_prompt_manager()

                # Try specific agent prompt first, then default
                prompt = await prompt_manager.get_prompt(f"{agent_id}-system")
                if not prompt:
                    prompt = await prompt_manager.get_default_prompt(PromptType.AGENT)

                if prompt:
                    # Build context from agent config
                    context = f"Agente: {agent_name}\nDescrição: {agent_description}"
                    if agent_config:
                        context += f"\nConfiguração: {agent_config}"

                    system_message = prompt.compile(context=context)
                    logger.debug(
                        "prompt_loaded_from_manager",
                        prompt_id=prompt.id,
                        agent_id=agent_id,
                    )
            except Exception as e:
                logger.warning("prompt_manager_fallback", error=str(e))

        # Fallback to hardcoded prompt
        if not system_message:
            system_message = (
                f"Você é {agent_name}, {agent_description}. "
                "Responda de forma útil e profissional em português brasileiro. "
                "Seja conciso e forneça informações precisas."
            )

        messages: list[dict[str, str]] = [{"role": "system", "content": system_message}]
        if conversation_history:
            messages.extend(conversation_history[-5:])  # últimas 5

        messages.append({"role": "user", "content": user_message})

        # Use tracer if available
        if LANGFUSE_INTEGRATION:
            tracer = get_tracer()
            async with tracer.trace("generate_agent_response", model=self.model) as trace:
                response = await self.generate_response(messages, max_tokens=800)
                trace.output = response
                trace.input_tokens = sum(len(m.get("content", "").split()) * 2 for m in messages)
                trace.output_tokens = len(response.split()) * 2
                return response

        return await self.generate_response(messages, max_tokens=800)

    async def generate_rag_response(
        self,
        query: str,
        context: str,
        conversation_history: list[dict[str, str]] | None = None,
        source_name: str = "the documentation",
        use_opinion_based: bool = True,
    ) -> str:
        """
        Generate a response using RAG (Retrieval-Augmented Generation).

        Now uses Opinion-Based Prompting by default for +30-50% improvement in
        context adherence. Research shows this technique improves accuracy from
        33% → 73% (120% improvement) by reformulating questions to force LLM
        to prioritize provided context over training data.

        Args:
            query: User's query
            context: Retrieved context/documents
            conversation_history: Previous conversation history
            source_name: Name of the knowledge source (e.g., "TWS manual")
            use_opinion_based: Use opinion-based prompting (recommended: True)

        Returns:
            Generated RAG response
        """
        # Use Opinion-Based Prompting for better accuracy
        if use_opinion_based:
            # Format with opinion-based technique
            formatted = self._prompt_formatter.format_rag_prompt(
                query=query,
                context=context,
                source_name=source_name,
                strict_mode=True,
                language="pt"  # Default to Portuguese for Resync
            )
            
            messages: list[dict[str, str]] = [
                {"role": "system", "content": formatted["system"]},
            ]
            
            # Add conversation history if provided
            if conversation_history:
                messages.extend(conversation_history[-3:])
            
            messages.append({"role": "user", "content": formatted["user"]})
            
        else:
            # Traditional approach (fallback, lower accuracy)
            # Try to get prompt from LangFuse/PromptManager
            system_message = None

            if LANGFUSE_INTEGRATION:
                try:
                    prompt_manager = get_prompt_manager()
                    prompt = await prompt_manager.get_default_prompt(PromptType.RAG)

                    if prompt:
                        system_message = prompt.compile(rag_context=context)
                        logger.debug("rag_prompt_loaded_from_manager", prompt_id=prompt.id)
                except Exception as e:
                    logger.warning("rag_prompt_manager_fallback", error=str(e))

            # Fallback to hardcoded prompt
            if not system_message:
                system_message = (
                    "Você é um assistente de IA especializado em responder perguntas "
                    "baseado no contexto fornecido. Use as informações do contexto para "
                    "responder de forma precisa e útil. Se o contexto não contiver "
                    "informações suficientes, diga que não sabe. Responda em português brasileiro.\n\n"
                    f"Contexto relevante:\n{context}"
                )

            messages: list[dict[str, str]] = [{"role": "system", "content": system_message}]
            if conversation_history:
                messages.extend(conversation_history[-3:])

            messages.append({"role": "user", "content": query})

        # Use tracer if available
        if LANGFUSE_INTEGRATION:
            tracer = get_tracer()
            async with tracer.trace(
                "generate_rag_response", 
                model=self.model, 
                prompt_id="rag",
                metadata={"opinion_based": use_opinion_based}
            ) as trace:
                response = await self.generate_response(messages, max_tokens=1000)
                trace.output = response
                trace.input_tokens = sum(len(m.get("content", "").split()) * 2 for m in messages)
                trace.output_tokens = len(response.split()) * 2
                return response

        return await self.generate_response(messages, max_tokens=1000)

    async def health_check(self) -> dict[str, Any]:
        """
        Perform a health check on LLM service

        Returns:
            Dictionary containing health status information
        """
        try:
            test_response = await self.generate_response(
                messages=[{"role": "user", "content": "Responda apenas com 'OK'."}],
                max_tokens=10,  # keep explicit (NVIDIA)
            )
            ok = test_response.strip() == "OK"
            status = "healthy" if ok else "degraded"
            result: dict[str, Any] = {
                "status": status,
                "model": self.model,
                "endpoint": getattr(settings, "llm_endpoint", None),
                "test_response": test_response,
            }
            if not ok:
                result["error"] = "Unexpected response"
            return result
        except (
            AuthenticationError,
            RateLimitError,
            APIConnectionError,
            BadRequestError,
            APIError,
            APITimeoutError,
            APIStatusError,
        ) as exc:
            return {
                "status": "unhealthy",
                "model": self.model,
                "endpoint": getattr(settings, "llm_endpoint", None),
                "error": str(exc),
                "request_id": getattr(exc, "request_id", None),
            }
        except Exception as exc:  # pylint: disable=broad-exception-caught
            logger.error("exception_caught", error=str(exc), exc_info=True)
            return {
                "status": "unhealthy",
                "model": self.model,
                "endpoint": getattr(settings, "llm_endpoint", None),
                "error": str(exc),
            }

    async def aclose(self) -> None:
        """Close underlying HTTP resources of the OpenAI client."""
        if hasattr(self.client, "close"):
            self.client.close()
        # AsyncOpenAI v1 doesn't have explicit close method
        # Resources are cleaned up automatically when the client is garbage collected


# Singleton idempotente, sem globals mutáveis
@lru_cache(maxsize=1)
def get_llm_service() -> LLMService:
    """Get or create global LLM service instance."""
    return LLMService()

