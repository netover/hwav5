# LiteLLM Configuration for Resync TWS Application
# v5.2.3.21: Added Ollama support for CPU-only local inference
#
# IMPORTANT: API keys MUST be provided via environment variables:
# - OPENAI_API_KEY (for fallback)
# - OPENROUTER_API_KEY (optional)
# - ANTHROPIC_API_KEY (optional)
#
# NEVER hardcode API keys in this file!

model_list:
  # =========================================================================
  # PRIMARY: Ollama Local Models (CPU-Only, Zero Cost)
  # =========================================================================
  
  # Qwen 2.5 3B - Primary model for TWS operations
  # Optimized for: Intent detection, JSON extraction, short responses
  # Performance: ~6-12 tokens/sec on 4-core CPU
  - model_name: "qwen-local"
    litellm_params:
      model: ollama/qwen2.5:3b
      api_base: http://localhost:11434
      temperature: 0.1        # Low temperature for precise TWS responses
      max_tokens: 512         # Limit output for faster response
      num_ctx: 4096           # Context window
      timeout: 8              # Aggressive timeout for fast fallback
      stream: true            # Enable streaming for better UX

  # Qwen 2.5 7B - For complex reasoning (if resources allow)
  - model_name: "qwen-large"
    litellm_params:
      model: ollama/qwen2.5:7b
      api_base: http://localhost:11434
      temperature: 0.2
      max_tokens: 1024
      num_ctx: 4096
      timeout: 15             # 7B is slower

  # Llama 3.2 3B - Alternative local model
  - model_name: "llama-local"
    litellm_params:
      model: ollama/llama3.2:3b
      api_base: http://localhost:11434
      temperature: 0.1
      max_tokens: 512
      num_ctx: 4096
      timeout: 8

  # =========================================================================
  # FALLBACK: Cloud Models (Used when local times out)
  # =========================================================================

  # GPT-4o-mini - Fast, cheap cloud fallback
  - model_name: "gpt-fallback"
    litellm_params:
      model: gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
      temperature: 0.1
      max_tokens: 1024
      timeout: 30             # Cloud can take longer

  # GPT-3.5-turbo - Cheapest cloud option
  - model_name: "gpt-cheap"
    litellm_params:
      model: gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY
      temperature: 0.1
      max_tokens: 1024
      timeout: 30

  # OpenRouter - Free tier fallback
  - model_name: "openrouter-free"
    litellm_params:
      model: meta-llama/llama-3.2-3b-instruct:free
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1
      temperature: 0.1
      max_tokens: 1024
      timeout: 30

# Model aliases for TWS use cases
model_aliases:
  # Primary (local)
  "tws-default": "qwen-local"
  "tws-intent": "qwen-local"
  "tws-json": "qwen-local"
  
  # Complex reasoning
  "tws-reasoning": "qwen-large"
  "tws-troubleshooting": "qwen-large"
  
  # Fallbacks
  "tws-fallback": "gpt-fallback"
  "tws-cheap": "gpt-cheap"

# Router settings for model selection
router_settings:
  # Model group aliases for different use cases
  model_group_alias:
    - group_name: "tws-general"
      list_of_model_names: ["qwen-local", "gpt-fallback", "gpt-cheap"]
    - group_name: "tws-troubleshooting"
      list_of_model_names: ["qwen-large", "qwen-local", "gpt-fallback"]
    - group_name: "tws-research"
      list_of_model_names: ["qwen-large", "gpt-fallback"]

  # Default fallback settings
  fallbacks:
    - type: "model"
      fallbacks: ["gpt-fallback", "gpt-cheap", "openrouter-free"]
  
  # Timeout settings - v5.2.3.21: Aggressive for local
  timeout: 8                  # 8 seconds for local models
  stream_timeout: 60          # Allow longer for streaming responses
  
  # Retry settings
  num_retries: 1              # Fast fallback, minimal retries
  retry_after: 0.5            # Quick retry

# Custom settings for TWS-specific usage
general_settings:
  # Enable caching for faster repeated queries
  cache:
    type: "redis"
    host: os.environ/REDIS_URL
    enabled: true
    ttl: 3600                 # Cache for 1 hour
  
  # Security - master key from environment
  master_key: os.environ/LITELLM_MASTER_KEY
  
  # Cost tracking - database URL from environment
  database_url: os.environ/DATABASE_URL
  
  # v5.2.3.21: Suppress verbose logging in production
  set_verbose: false

# Environment variable defaults (for documentation)
environment_variables:
  OLLAMA_HOST: "http://localhost:11434"
  OLLAMA_NUM_PARALLEL: "1"    # Single request at a time (CPU constraint)
  OLLAMA_MAX_LOADED_MODELS: "1"
  OLLAMA_FLASH_ATTENTION: "1"
  OLLAMA_KV_CACHE_TYPE: "q8_0"