#!/usr/bin/env python3
"""
Test script for Ollama/Qwen integration with LiteLLM.

v5.2.3.21: Validates local LLM setup and fallback behavior.

Usage:
    # Start Ollama first
    ollama serve
    ollama pull qwen2.5:3b
    
    # Run tests
    python scripts/test_ollama_integration.py

Requirements:
    - Ollama running on localhost:11434
    - Model qwen2.5:3b pulled
    - OPENAI_API_KEY set (for fallback testing)
"""

import asyncio
import json
import os
import sys
import time
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))


async def test_ollama_health():
    """Test if Ollama is running and accessible."""
    import httpx
    
    print("\n" + "=" * 60)
    print("ğŸ” Test 1: Ollama Health Check")
    print("=" * 60)
    
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get("http://localhost:11434/api/tags")
            if response.status_code == 200:
                data = response.json()
                models = [m["name"] for m in data.get("models", [])]
                print(f"âœ… Ollama estÃ¡ rodando!")
                print(f"   Modelos disponÃ­veis: {models}")
                
                if any("qwen" in m.lower() for m in models):
                    print(f"   âœ… Qwen encontrado!")
                    return True
                else:
                    print(f"   âš ï¸  Qwen nÃ£o encontrado. Execute: ollama pull qwen2.5:3b")
                    return False
            else:
                print(f"âŒ Ollama retornou status {response.status_code}")
                return False
    except Exception as e:
        print(f"âŒ Ollama nÃ£o estÃ¡ acessÃ­vel: {e}")
        print("   Execute: ollama serve")
        return False


async def test_litellm_ollama():
    """Test LiteLLM with Ollama provider."""
    print("\n" + "=" * 60)
    print("ğŸ” Test 2: LiteLLM + Ollama Integration")
    print("=" * 60)
    
    try:
        import litellm
        litellm.suppress_debug_info = True
        
        start = time.time()
        response = await litellm.acompletion(
            model="ollama/qwen2.5:3b",
            messages=[{"role": "user", "content": "Responda apenas: OK"}],
            api_base="http://localhost:11434",
            max_tokens=10,
            temperature=0.1,
        )
        elapsed = time.time() - start
        
        content = response.choices[0].message.content
        print(f"âœ… LiteLLM + Ollama funcionando!")
        print(f"   Resposta: {content}")
        print(f"   Tempo: {elapsed:.2f}s")
        return True
        
    except Exception as e:
        print(f"âŒ Falha no LiteLLM + Ollama: {e}")
        return False


async def test_streaming():
    """Test streaming with Ollama."""
    print("\n" + "=" * 60)
    print("ğŸ” Test 3: Streaming")
    print("=" * 60)
    
    try:
        import litellm
        litellm.suppress_debug_info = True
        
        print("   Resposta (streaming): ", end="", flush=True)
        start = time.time()
        
        response = await litellm.acompletion(
            model="ollama/qwen2.5:3b",
            messages=[{"role": "user", "content": "Conte atÃ© 5 em portuguÃªs"}],
            api_base="http://localhost:11434",
            max_tokens=50,
            temperature=0.1,
            stream=True,
        )
        
        chunks = []
        async for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                chunks.append(content)
                print(content, end="", flush=True)
        
        elapsed = time.time() - start
        print(f"\n   âœ… Streaming OK! ({elapsed:.2f}s)")
        return True
        
    except Exception as e:
        print(f"\nâŒ Falha no streaming: {e}")
        return False


async def test_json_mode():
    """Test JSON mode with Qwen."""
    print("\n" + "=" * 60)
    print("ğŸ” Test 4: JSON Mode")
    print("=" * 60)
    
    try:
        import litellm
        litellm.suppress_debug_info = True
        
        prompt = """Extraia o nome do job da seguinte frase:
"Por favor cancele o job PAYMENT_PROCESS_DAILY"

Responda APENAS com JSON no formato: {"job_name": "nome"}"""
        
        start = time.time()
        response = await litellm.acompletion(
            model="ollama/qwen2.5:3b",
            messages=[{"role": "user", "content": prompt}],
            api_base="http://localhost:11434",
            max_tokens=50,
            temperature=0.1,
            format="json",
        )
        elapsed = time.time() - start
        
        content = response.choices[0].message.content
        print(f"   Resposta raw: {content}")
        
        # Try to parse JSON
        try:
            parsed = json.loads(content)
            print(f"   âœ… JSON vÃ¡lido: {parsed}")
            print(f"   Tempo: {elapsed:.2f}s")
            return True
        except json.JSONDecodeError:
            print(f"   âš ï¸  JSON invÃ¡lido (comum em modelos 3B)")
            return False
        
    except Exception as e:
        print(f"âŒ Falha no JSON mode: {e}")
        return False


async def test_fallback_service():
    """Test the LLMService with fallback."""
    print("\n" + "=" * 60)
    print("ğŸ” Test 5: LLMService com Fallback")
    print("=" * 60)
    
    try:
        from resync.services.llm_fallback import get_llm_service
        
        llm = await get_llm_service()
        print(f"   Primary model: {llm.config.primary_model}")
        print(f"   Fallback chain: {llm.config.fallback_chain}")
        print(f"   Timeout: {llm.config.default_timeout}s")
        
        start = time.time()
        response = await llm.complete(
            prompt="Qual Ã© a capital do Brasil? Responda em uma palavra.",
            max_tokens=20,
        )
        elapsed = time.time() - start
        
        print(f"   âœ… Resposta: {response.content}")
        print(f"   Provider: {response.provider.value}")
        print(f"   Fallback: {response.was_fallback}")
        print(f"   Tempo: {elapsed:.2f}s")
        return True
        
    except Exception as e:
        print(f"âŒ Falha no LLMService: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_timeout_fallback():
    """Test that timeout triggers fallback to cloud."""
    print("\n" + "=" * 60)
    print("ğŸ” Test 6: Timeout â†’ Fallback (simulado)")
    print("=" * 60)
    
    # This test is informational only
    print("   â„¹ï¸  Para testar o fallback real:")
    print("   1. Pare o Ollama: systemctl stop ollama")
    print("   2. FaÃ§a uma chamada via LLMService")
    print("   3. Deve usar gpt-4o-mini automaticamente")
    print("   4. Reinicie: systemctl start ollama")
    
    if os.getenv("OPENAI_API_KEY"):
        print("   âœ… OPENAI_API_KEY configurada - fallback funcionarÃ¡")
        return True
    else:
        print("   âš ï¸  OPENAI_API_KEY nÃ£o configurada - fallback nÃ£o funcionarÃ¡")
        return False


async def test_performance():
    """Test token generation speed."""
    print("\n" + "=" * 60)
    print("ğŸ” Test 7: Performance (tokens/segundo)")
    print("=" * 60)
    
    try:
        import litellm
        litellm.suppress_debug_info = True
        
        # Generate longer response to measure throughput
        start = time.time()
        response = await litellm.acompletion(
            model="ollama/qwen2.5:3b",
            messages=[{"role": "user", "content": "Explique o que Ã© TWS em 100 palavras"}],
            api_base="http://localhost:11434",
            max_tokens=150,
            temperature=0.1,
        )
        elapsed = time.time() - start
        
        content = response.choices[0].message.content
        tokens = len(content.split())  # AproximaÃ§Ã£o
        tokens_per_sec = tokens / elapsed
        
        print(f"   Tokens gerados: ~{tokens}")
        print(f"   Tempo total: {elapsed:.2f}s")
        print(f"   Velocidade: ~{tokens_per_sec:.1f} tokens/s")
        
        if tokens_per_sec >= 5:
            print(f"   âœ… Performance aceitÃ¡vel")
        else:
            print(f"   âš ï¸  Performance baixa (esperado em CPU)")
        
        return True
        
    except Exception as e:
        print(f"âŒ Falha no teste de performance: {e}")
        return False


async def main():
    """Run all tests."""
    print("\n" + "=" * 60)
    print("ğŸš€ Resync v5.2.3.21 - Ollama Integration Tests")
    print("=" * 60)
    
    results = {}
    
    # Basic health check
    results["ollama_health"] = await test_ollama_health()
    
    if not results["ollama_health"]:
        print("\nâŒ Ollama nÃ£o estÃ¡ disponÃ­vel. Abortando testes.")
        print("   Execute: ollama serve && ollama pull qwen2.5:3b")
        return
    
    # LiteLLM integration
    results["litellm"] = await test_litellm_ollama()
    
    # Streaming
    results["streaming"] = await test_streaming()
    
    # JSON mode
    results["json"] = await test_json_mode()
    
    # LLMService
    results["service"] = await test_fallback_service()
    
    # Fallback check
    results["fallback"] = await test_timeout_fallback()
    
    # Performance
    results["performance"] = await test_performance()
    
    # Summary
    print("\n" + "=" * 60)
    print("ğŸ“Š Resumo dos Testes")
    print("=" * 60)
    
    passed = sum(1 for v in results.values() if v)
    total = len(results)
    
    for test, result in results.items():
        status = "âœ…" if result else "âŒ"
        print(f"   {status} {test}")
    
    print(f"\n   Total: {passed}/{total} testes passaram")
    
    if passed == total:
        print("\nğŸ‰ Todos os testes passaram! Ollama estÃ¡ pronto para uso.")
    else:
        print("\nâš ï¸  Alguns testes falharam. Verifique a configuraÃ§Ã£o.")


if __name__ == "__main__":
    asyncio.run(main())
